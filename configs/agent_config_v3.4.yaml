# Enterprise Coding Agent v3.4 Configuration
# Modular, DRY design for multi-domain workflows. GPT-5-Codex primary for agentic tasks.
# Generated September 15, 2025. Load with ruamel.yaml for advanced features.

default_model_config: &default_config
  timeout: 2100  # Max for adaptive thinking
  retry: 3  # Backoff retries
  temperature: 0.2  # Low for determinism

components: &components
  models: &models
    planner: &planner_model claude_sonnet_4  # Claude for decomposition (maximize Anthropic Max value)
    coder_primary: &coder_primary_model claude_sonnet_4  # Claude primary for generation/reflection
    coder_backup: &coder_backup_model openai_gpt_5  # OpenAI as backup only (using gpt-3.5-turbo)
    reviewer_a: &reviewer_a_model claude_sonnet_4  # Claude for primary review
    reviewer_b: &reviewer_b_model claude_opus_4  # Claude Opus for complex/critical reviews
    reflector_primary: &reflector_primary_model claude_sonnet_4  # Claude for fixes and improvements
    reflector_secondary: &reflector_secondary_model openai_gpt_5  # OpenAI backup for library awareness
    validator: &validator_model claude_sonnet_4  # Claude for validation
    emerging: &emerging_models
      - google_gemini_2_5  # Optional multimodal
      # TODO: Add more as available
  tools: &tools
    code_generator: &code_generator_tools
      cursor: &cursor_tool cursor_ai_composer  # IDE integration
      swarm: &swarm_tool baidu_genflow_2_0_coding_agent  # Parallelism
    ui_automation: &ui_automation_tool openai_computer_use  # Multimodal UI
    semantic_search: &semantic_search_tool sourcegraph_cody  # Code retrieval
    embeddings: &embeddings_tool codet5_plus  # Vector similarity
    refactoring: &refactoring_tool pattern_trained  # Automated refactors
    emerging: &emerging_tools
      - name: github_copilot_workspace  # Collaboration
      - name: replit_agent  # Execution
      - name: advanced_rag
        provider: pinecone_weaviate_multimodal  # Retrieval
      - name: security_agents
        provider: snyk_security_cli  # Vulns
        config:
          project_id: "39fe3efb-d65a-4f29-a662-7659fffc4476"  # Snyk project ID
      - name: genflow_2_0_swarm
        provider: baidu_multi_agent_parallel  # Swarms
      - name: gpt_5_codex_cli
        provider: openai_agentic_coding  # Primary CLI for auto-edit
      - name: claude_code_cli
        provider: anthropic_agentic_coding  # Backup CLI
    optimization: &optimization_tools
      core: [search, test_runner, graph_query]
      eval: quarterly_transcripts  # Benchmarking

enterprise_coding_agent:
  planning:
    model: *planner_model
    reasoning: persistent_chain  # Maintain context
    prompt_adapter: "Think step-by-step"  # Domain adapters can override
    <<: *default_config
  coding:
    primary: *coder_primary_model
    backup: *coder_backup_model
    specialists:
      - *cursor_tool
      - *ui_automation_tool
      - *swarm_tool
      - {name: bug_finder, value: ensemble_validator_reflector}
      - {name: gpt_5_codex_integration, value: openai_agentic_coding}
      - {name: claude_code_integration, value: anthropic_agentic_coding}
    <<: *default_config
  reviewing:
    ensemble:
      - *reviewer_a_model
      - *reviewer_b_model
    confidence_threshold: 0.8  # Halt/fix below
  reflecting:
    models:
      - name: *reflector_primary_model
        adapter: multimodal_stack_trace  # Traces
      - name: *reflector_secondary_model
        adapter: real_time_library_awareness  # Libs
    ensemble: correction_plan  # Combine fixes
    prompt: debugger_mode  # Focused debugging
    trigger: test_failure_threshold > 0.5
    max_iterations: 5  # Self-healing limit (configurable via REFLECTION_MAX_ITERATIONS env var)
    halt_on_find: true  # Stop if confidence >=0.8
    confidence_threshold: 0.8  # Minimum confidence to halt reflection (configurable via REFLECTION_CONFIDENCE_THRESHOLD env var)
    early_termination:
      enable: true  # Enable early termination heuristics
      stagnation_threshold: 3  # Stop if no improvement for N iterations
      min_iterations: 1  # Minimum iterations before early termination
      progress_threshold: 0.1  # Minimum confidence improvement required
    gpt_5_codex_enhance: true  # Primary CLI
    claude_code_enhance: conditional  # Vuln only
    <<: *default_config
  validating:
    model: *validator_model
    prompt: test_plan_generation
    trigger: post_planning
    coverage_threshold: 0.97  # Strict for quality
    tools:
      - unit
      - integration
      - fuzz
  orchestration:
    framework: "langgraph + genflow_multi_agent_parallel"  # Graph + swarms
    routing: dynamic_cost_complexity  # Codex if <0.05, Claude on vuln
    parallel_execution: enabled  # 60-80% faster
    self_healing: true  # Retries
    human_in_loop: genflow_swarm_refinements  # HITL
    modes:
      - build_from_docs: greenfield_from_scratch  # Coding default
      - scan_fix_existing: end_to_end_scan_with_decisions  # Social scan
    runtime_optimizer:
      cost_aware: true
      dynamic_benchmarks: swe_bench_microtasks_periodic  # Eval
    parallelism_guardrails:
      agent_governor: throttle_on_cost_spike_or_drift  # Caps
    decision_layer:
      triggers:
        bug_rate: "> 0.25"
        complexity: "> 18"
        maintainability: "< 85"
      actions:
        - fix_targeted
        - delete_refactor_parts
        - rebuild_from_scratch
  memory:
    types:
      - immediate  # Task-local
      - session  # Run
      - project  # Persistent
      - organizational  # Shared
    storage: hybrid(vector+graph+relational)  # Pinecone + NetworkX
    knowledge_graph: gitlab_rust_based  # Deps
    decay_layer: relevance_based_pruning_agent  # Prune 40%
  caching:
    enabled: true  # Enable caching system (configurable via CACHE_ENABLED env var)
    default_ttl: 600  # 10 minutes default (configurable via CACHE_DEFAULT_TTL env var)
    max_size: 1000  # Maximum cache entries (configurable via CACHE_MAX_SIZE env var)
    cleanup_interval: 60  # Cleanup every minute (configurable via CACHE_CLEANUP_INTERVAL env var)
    adaptive_ttl: true  # Enable adaptive TTL based on quality scores
    quality_threshold: 0.8  # Minimum quality for extended TTL
    high_quality_ttl_multiplier: 2.0  # Multiply TTL for high-quality items
    low_quality_ttl_multiplier: 0.5  # Reduce TTL for low-quality items
    persistence_enabled: false  # Enable disk persistence (configurable via CACHE_PERSISTENCE_ENABLED env var)
    persistence_path: ".cache"  # Path for persistent cache files
    compression_enabled: true  # Enable compression for large values
    compression_threshold: 1024  # Compress values larger than this (bytes)
    eviction_policy: "lru"  # "lru", "lfu", or "ttl" (configurable via CACHE_EVICTION_POLICY env var)
    metrics_enabled: true  # Enable detailed metrics collection
    model_cache:
      enabled: true  # Enable model response caching
      default_ttl: 900  # 15 minutes for model responses
      max_size: 500  # Model cache size
      claude_ttl_multiplier: 1.5  # Longer TTL for Claude responses (subscription value)
      high_confidence_ttl_multiplier: 2.0  # Longer TTL for high-confidence responses
  intelligence:
    semantic_search: *semantic_search_tool
    dependency_analysis: enabled
    embeddings: *embeddings_tool
    automated_refactoring: *refactoring_tool
  execution:
    environment: "nix_flakes + wasm_sandbox"  # Secure
  security:
    features:
      - zero_trust_execution
      - ml_vulnerability_detection
      - supply_chain_scanning
      - behavioral_monitoring
      - cycode_red_teaming
      - genflow_anomaly
    gates:
      cycode_red_teaming: pre_merge_gate
      genflow_anomaly: swarm_level_checks
    retention_days: 30  # Compliance default
  observability:
    features:
      - quality_metrics
      - decision_traces
      - developer_benchmarking
      - agentops_tracing  # LLM calls
      - genflow_telemetry
    telemetry:
      genflow_telemetry: sub_agent_logs
    metrics:
      enabled: true  # Enable comprehensive metrics collection (configurable via METRICS_ENABLED env var)
      buffer_size: 10000  # Maximum metrics to buffer before flushing
      flush_interval: 60.0  # Flush metrics every 60 seconds
      export_path: ".metrics"  # Path to export metrics files (configurable via METRICS_EXPORT_PATH env var)
      collect_system_metrics: true  # Collect system performance metrics
      collect_performance_metrics: true  # Collect execution timing metrics
      collect_error_metrics: true  # Collect error and failure metrics
      collect_model_metrics: true  # Collect model call metrics
      collect_reflection_metrics: true  # Collect reflection loop metrics
      aggregation:
        histogram_buckets: [0.1, 0.5, 1.0, 2.5, 5.0, 10.0]  # Histogram buckets for timing
        percentiles: [0.5, 0.95, 0.99]  # Percentiles to calculate
        retention_hours: 24  # Hours to retain detailed metrics
  governance:
    metrics:
      - code_quality: "sonarqube (complexity, duplication, maintainability >85)"
      - performance: locust_benchmarking
      - success_rate: "pr_merge_velocity (auto vs approved)"
      - swarm_efficiency: genflow_completion_rates
      - business_impact: velocity_lead_time_spec_to_deploy
      - bug_severity: sonarqube_scoring
    rebuild_thresholds:
      bug_rate: 0.25
      complexity: 18
      maintainability: 85
    alerting: grafana_dashboards
  tools:
    emerging: *emerging_tools
    optimization: *optimization_tools
  models_emerging: *emerging_models

# Schema Notes: Extend with &default_config for shared timeouts/retries. Validate with ruamel.yaml.


